{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a117672",
   "metadata": {},
   "source": [
    "\n",
    "# BASIC CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "570888c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n",
      "WordNet successfully loaded - lemmatization available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\karth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\karth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\karth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download essential NLTK resources\n",
    "print(\"Downloading NLTK data...\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Try to download wordnet - used for lemmatization\n",
    "try:\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatization_available = True\n",
    "    print(\"WordNet successfully loaded - lemmatization available\")\n",
    "except:\n",
    "    lemmatization_available = False\n",
    "    print(\"WordNet could not be loaded - proceeding without lemmatization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5214791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment variables configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Create and load .env file if it doesn't exist\n",
    "if not os.path.exists('.env'):\n",
    "    with open('.env', 'w') as f:\n",
    "        f.write(\"# Environment variables for models\\n\")\n",
    "        f.write(\"TRANSFORMERS_HOME=./models\\n\")\n",
    "        f.write(\"CHROMADB_PATH=./chroma_db\\n\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "os.makedirs('./chroma_db', exist_ok=True)\n",
    "\n",
    "print(\"environment variables configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e44da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dateparser\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64b8de3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File 'SMData.xlsx' not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = 'SMData.xlsx'\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(dataset_path)\n",
    "    print(f\"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "    print(\"\\nDataset columns:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"- {col}\")\n",
    "    print(\"\\nSample data:\")\n",
    "    display(df.head(3))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{dataset_path}' not found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6580a359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying installations and initializing models...\n",
      "Sentence Transformer model 'all-MiniLM-L6-v2' loaded successfully.\n",
      "ChromaDB client initialized successfully.\n",
      "\n",
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying installations and initializing models...\")\n",
    "\n",
    "# Initialize embedding model\n",
    "try:\n",
    "    model_name = 'all-MiniLM-L6-v2'  # Small but effective model for embeddings\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "    print(f\"Sentence Transformer model '{model_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading embedding model: {str(e)}\")\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "try:\n",
    "    # Using PersistentClient for local storage\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\", settings=Settings())\n",
    "    print(\"ChromaDB client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing ChromaDB client: {str(e)}\")\n",
    "    \n",
    "print(\"\\nEnvironment setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "294bebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGen package loaded successfully.\n",
      "Python version: 3.11.0\n",
      "Operating System: Windows 10\n",
      "Current date: 2025-05-12 21:46:57\n",
      "\n",
      "All systems ready for the Agentic RAG implementation!\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "\n",
    "# Verify AutoGen is working\n",
    "try:\n",
    "    from autogen import AssistantAgent, UserProxyAgent\n",
    "    print(\"AutoGen package loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading AutoGen: {str(e)}\")\n",
    "\n",
    "\n",
    "# Display system information - helpful for debugging\n",
    "\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"Operating System: {platform.system()} {platform.release()}\")\n",
    "print(f\"Current date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nAll systems ready for the Agentic RAG implementation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252373c0",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67462fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text_columns = ['Next Steps', 'Outcome of meeting']\n",
    "\n",
    "\n",
    "# Text cleaning function with fallback options\n",
    "def clean_text(text, use_lemmatizer=True):\n",
    "    \"\"\"Clean and normalize text data\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}. Using basic split instead.\")\n",
    "        tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords if available\n",
    "    try:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    except Exception as e:\n",
    "        print(f\"Stopwords not available: {e}. Skipping stopword removal.\")\n",
    "    \n",
    "    # Lemmatize if available and requested\n",
    "    if lemmatization_available and use_lemmatizer:\n",
    "        try:\n",
    "            tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        except Exception as e:\n",
    "            print(f\"Lemmatization error: {e}. Using original tokens.\")\n",
    "    \n",
    "    # Join back to string\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea5a7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text columns\n",
    "def process_text_columns(df):\n",
    "    \"\"\"Clean text data in specified columns\"\"\"\n",
    "    text_columns = ['Next Steps', 'Outcome of meeting']\n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            print(f\"Cleaning text in {col}...\")\n",
    "            df[col] = df[col].apply(lambda x: clean_text(x, use_lemmatizer=lemmatization_available))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfcbc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract key entities\n",
    "def extract_entities(df):\n",
    "    \"\"\"Extract key entities like customer names and products\"\"\"\n",
    "    entities = {}\n",
    "    \n",
    "    # Extract customer names\n",
    "    if 'Visit Plan: Owner Name Customer' in df.columns:\n",
    "        entities['customers'] = df['Visit Plan: Owner Name Customer'].dropna().unique().tolist()\n",
    "    elif 'Customer' in df.columns:\n",
    "        entities['customers'] = df['Customer'].dropna().unique().tolist()\n",
    "    else:\n",
    "        entities['customers'] = []\n",
    "    \n",
    "    # Extract products\n",
    "    if 'Visit Plan: Product Division' in df.columns:\n",
    "        entities['products'] = df['Visit Plan: Product Division'].dropna().unique().tolist()\n",
    "    else:\n",
    "        entities['products'] = []\n",
    "    \n",
    "    print(f\"Extracted {len(entities['customers'])} unique customers and {len(entities['products'])} unique products.\")\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "734fad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle missing values\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Handle missing values in the DataFrame\"\"\"\n",
    "    # Fill missing values in text columns with empty string\n",
    "    text_columns = ['Next Steps', 'Outcome of meeting']\n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(\"\")\n",
    "    \n",
    "    # Fill missing values in categorical columns with 'Unknown'\n",
    "    categorical_columns = ['Visit Plan: Owner Region', 'Visit Plan: Product Division']\n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(\"Unknown\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e43b9327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize formats\n",
    "def standardize_formats(df):\n",
    "    \"\"\"Standardize formats across columns\"\"\"\n",
    "    # Strip whitespace from string columns\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if hasattr(df[col], 'str'):  # Check if it's a string column\n",
    "            df[col] = df[col].str.strip()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a22daa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main preprocessing function\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Main function to preprocess the data\"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Apply preprocessing steps\n",
    "    # df_processed = convert_dates(df_processed)\n",
    "    df_processed = process_text_columns(df_processed)\n",
    "    entities = extract_entities(df_processed)\n",
    "    df_processed = handle_missing_values(df_processed)\n",
    "    df_processed = standardize_formats(df_processed)\n",
    "    \n",
    "    print(\"Data preprocessing completed successfully.\")\n",
    "    print (f\"Processed DataFrame shape: {df_processed.shape}\")\n",
    "    print(\"Sample processed data:\")\n",
    "    print(df_processed.head())\n",
    "    \n",
    "    return df_processed, entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f935f56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: DataFrame 'df' not found.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError: DataFrame \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m not found.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Process the data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df_processed, entities = preprocess_data(\u001b[43mdf\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df\n",
    "except NameError:\n",
    "    print(\"Error: DataFrame 'df' not found.\")\n",
    "\n",
    "# Process the data\n",
    "df_processed, entities = preprocess_data(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ce03e",
   "metadata": {},
   "source": [
    "\n",
    "# Vectorization & Embedding Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488525e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"Setting up vectorization and embedding creation pipeline...\")\n",
    "\n",
    "print(\"Loading the sentence transformer model...\")\n",
    "\n",
    "# all-MiniLM-L6-v2 - maps sentences to a 384-dimensional dense vector space\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "\n",
    "try:\n",
    "    # Check if CUDA is available for faster processing\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"  - Using device: {device}\")\n",
    "    \n",
    "    embedding_model = SentenceTransformer(model_name, device=device)\n",
    "    embedding_dimension = embedding_model.get_sentence_embedding_dimension()\n",
    "    print(f\"  - Successfully loaded '{model_name}' model\")\n",
    "    print(f\"  - Embedding dimension: {embedding_dimension}\")\n",
    "except Exception as e:\n",
    "    print(f\"  - Error loading model: {str(e)}\")\n",
    "    print(\"  - Falling back to a simpler approach...\")\n",
    "    embedding_dimension = 384  # Default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fbdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine text fields into a single representation\n",
    "def create_combined_text(row, text_columns=['Next Steps', 'Outcome of meeting'], \n",
    "                         metadata_columns=['Visit Plan: Owner Name', \n",
    "                                          'Customer', \n",
    "                                          'Visit Plan: Product Division', 'Customer SAP Code', 'Visit Plan: Visit Date',\n",
    "                                          'Visit Report: Created Date',\t'Visit Plan: Owner Region',\t'Visit Plan: Visit Owner Email']):\n",
    "\n",
    "    \"\"\"\n",
    "    Create a combined text representation from multiple columns in a row.\n",
    "    This is to prioritize text columns while adding context from metadata columns.\n",
    "    \"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    # Add text from main content columns\n",
    "    for col in text_columns:\n",
    "        if col in row and pd.notna(row[col]) and row[col]:\n",
    "            text_parts.append(f\"{col}: {row[col]}\")\n",
    "    \n",
    "    # Add context from metadata columns\n",
    "    metadata_parts = []\n",
    "    for col in metadata_columns:\n",
    "        if col in row and pd.notna(row[col]) and row[col]:\n",
    "            metadata_parts.append(f\"{col}: {row[col]}\")\n",
    "    \n",
    "    combined_text = \" \".join(text_parts)\n",
    "    \n",
    "    # Add metadata with a separator if we have both text and metadata\n",
    "    if text_parts and metadata_parts:\n",
    "        combined_text += \" | \" + \" | \\n\".join(metadata_parts)\n",
    "    elif metadata_parts:\n",
    "        combined_text = \" | \".join(metadata_parts)\n",
    "    \n",
    "    return combined_text\n",
    "\n",
    "# Testing for a sample row\n",
    "if not df.empty:\n",
    "    sample_combined = create_combined_text(df.iloc[0])\n",
    "    print(\"Creating combined text representations:\")\n",
    "    print(f\"  - Sample combined text (truncated): {sample_combined}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d77c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each row\n",
    "print(\"\\nGenerating embeddings for each row...\")\n",
    "\n",
    "# Create a new column for the combined text\n",
    "df['combined_text'] = df.apply(create_combined_text, axis=1)\n",
    "\n",
    "# Function to generate embeddings in batches\n",
    "def generate_embeddings_in_batches(texts, model, batch_size=32):\n",
    "    \"\"\"Generate embeddings for a list of texts in batches to avoid memory issues\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(batch_texts, show_progress_bar=False, \n",
    "                                        normalize_embeddings=True)\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "# Generate embeddings\n",
    "try:\n",
    "    # Get the list of combined texts\n",
    "    texts = df['combined_text'].tolist()\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = generate_embeddings_in_batches(texts, embedding_model)\n",
    "    \n",
    "    # Store embeddings in the dataframe\n",
    "    df['embedding'] = list(embeddings)\n",
    "    \n",
    "    print(f\"  - Successfully generated {len(embeddings)} embeddings\")\n",
    "    print(f\"  - Embedding shape: {embeddings[0].shape}\")\n",
    "    \n",
    "    # Show a sample embedding (first 5 dimensions)\n",
    "    if not df.empty:\n",
    "        sample_embedding = df['embedding'].iloc[0]\n",
    "        print(f\"  - Sample embedding (first 5 dimensions): {sample_embedding[:5]}\")\n",
    "except Exception as e:\n",
    "    print(f\"  - Error generating embeddings: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e34f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata for each embedding\n",
    "print(\"Creating metadata for each embedding...\")\n",
    "\n",
    "def create_metadata(row):\n",
    "    \"\"\"Create a metadata dictionary for a row\"\"\"\n",
    "    metadata = {}\n",
    "    \n",
    "    # Add date information\n",
    "    for date_col in ['Visit Plan: Visit Date', 'Visit Report: Created Date']:\n",
    "        if date_col in row and pd.notna(row[date_col]):\n",
    "            metadata[date_col.replace(':', '_')] = str(row[date_col])\n",
    "    \n",
    "    # Add key identifying information\n",
    "    for col in ['Visit Plan: Visit Owner Email', 'Customer', 'Visit Plan: Product Division', \n",
    "                'Visit Plan: Owner Region', 'Customer SAP Code', 'Visit Plan: Owner Name']:\n",
    "        if col in row and pd.notna(row[col]):\n",
    "            metadata[col.replace(':', '_')] = str(row[col])\n",
    "    \n",
    "    # Add the original text content\n",
    "    for col in ['Next Steps', 'Outcome of meeting']:\n",
    "        if col in row and pd.notna(row[col]):\n",
    "            metadata[col.replace(':', '_')] = str(row[col])\n",
    "    \n",
    "    # Add the combined text used for embedding\n",
    "    metadata['combined_text'] = row['combined_text']\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Create metadata for each row\n",
    "df['metadata'] = df.apply(create_metadata, axis=1)\n",
    "\n",
    "# Show a sample metadata\n",
    "if not df.empty:\n",
    "    sample_metadata = df['metadata'].iloc[0]\n",
    "    print(\"  - Sample metadata:\")\n",
    "    for key, value in list(sample_metadata.items()):  \n",
    "        print(f\"    - {key}: {value[:50]}...\" if isinstance(value, str) and len(value) > 50 else f\"    - {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "print(\"Setting up ChromaDB for vector storage...\")\n",
    "\n",
    "os.makedirs(\"./chroma_db\", exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Initialize ChromaDB client\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    \n",
    "    # Create an embedding function that matches our model\n",
    "    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
    "    \n",
    "    collection_name = \"customer_visits\"\n",
    "    \n",
    "    # Delete collection if it already exists (for clean restart)\n",
    "    try:\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "        print(f\"  - Deleted existing collection: {collection_name}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Create a new collection\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_function,\n",
    "        metadata={\"description\": \"Customer visit data with embeddings\"}\n",
    "    )\n",
    "    \n",
    "    print(f\"  - Successfully created ChromaDB collection: {collection_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"  - Error setting up ChromaDB: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Storing embeddings and metadata in ChromaDB...\")\n",
    "\n",
    "try:\n",
    "    # Prepare data for batch addition\n",
    "    ids = []\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    embeddings_list = []\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing data\"):\n",
    "        # Create a unique ID\n",
    "        unique_id = f\"doc_{idx}\"\n",
    "        \n",
    "        # Get the combined text\n",
    "        document = row['combined_text']\n",
    "        \n",
    "        # Get the metadata\n",
    "        metadata = row['metadata']\n",
    "        \n",
    "        # Get the embedding\n",
    "        embedding = row['embedding']\n",
    "        \n",
    "        # Add to lists\n",
    "        ids.append(unique_id)\n",
    "        documents.append(document)\n",
    "        metadatas.append(metadata)\n",
    "        embeddings_list.append(embedding)\n",
    "    \n",
    "    # Add data to ChromaDB in batches\n",
    "    batch_size = 100\n",
    "    for i in tqdm(range(0, len(ids), batch_size), desc=\"Adding to ChromaDB\"):\n",
    "        batch_ids = ids[i:i+batch_size]\n",
    "        batch_documents = documents[i:i+batch_size]\n",
    "        batch_metadatas = metadatas[i:i+batch_size]\n",
    "        batch_embeddings = embeddings_list[i:i+batch_size]\n",
    "        \n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            ids=batch_ids,\n",
    "            documents=batch_documents,\n",
    "            metadatas=batch_metadatas,\n",
    "            embeddings=batch_embeddings\n",
    "        )\n",
    "    \n",
    "    # Get collection count to verify\n",
    "    count = collection.count()\n",
    "    print(f\"  - Successfully stored {count} embeddings in ChromaDB\")\n",
    "except Exception as e:\n",
    "    print(f\"  - Error storing embeddings in ChromaDB: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing a simple query to verify the embeddings...\")\n",
    "\n",
    "try:\n",
    "    # Create a test query\n",
    "    test_query = \"what will rinac india do with the overdue?\"\n",
    "    \n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_texts=[test_query],\n",
    "        n_results=2,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Query: '{test_query}'\\n\")\n",
    "    print(f\"Found {len(results['documents'][0])} results\\n\")\n",
    "    \n",
    "    for i, (doc, metadata, distance) in enumerate(zip(\n",
    "        results['documents'][0], \n",
    "        results['metadatas'][0], \n",
    "        results['distances'][0]\n",
    "    )):\n",
    "        print(f\"Result {i+1} (similarity: {1-distance:.4f}):\")\n",
    "        print(f\"Document: {doc[:100]}...\")\n",
    "        print(f\"Customer: {metadata.get('Customer', 'N/A')}\")\n",
    "        print(f\"Product: {metadata.get('Visit Plan_ Product Division', 'N/A')}\")\n",
    "        # print(f\"Visit Plan: Owner Name: {metadata.get('Visit Plan_ Owner Name', 'N/A')}\")\n",
    "        # print(f\"Outcome of meeting: {metadata.get('Outcome of meeting', 'N/A')}\")\n",
    "        # print(f\"Visit Plan: Customer SAP: {metadata.get('Customer SAP Code', 'N/A')}\")\n",
    "        # print(f\"Visit Plan: Owner Region: {metadata.get('Visit Plan_ Owner Region', 'N/A')}\")\n",
    "        # print(f\"Visit Plan: Visit Owner Email: {metadata.get('Visit Plan_ Visit Owner Email', 'N/A')}\")\n",
    "        # print(f\"Visit Report: Created Date: {metadata.get('Visit Report_ Created Date\\n', 'N/A\\n' )}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  - Error testing query: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba81a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving processed data...\")\n",
    "\n",
    "try:\n",
    "    # Save the dataframe without the embedding column (to save space)\n",
    "    df_to_save = df.drop(columns=['embedding'])\n",
    "    \n",
    "    df_to_save.to_csv(\"processed_customer_visits.csv\", index=False)\n",
    "    print(\"Saved processed data to 'processed_customer_visits.csv'\")\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    # Create a mapping of document IDs to their metadata\n",
    "    metadata_mapping = {f\"doc_{idx}\": row['metadata'] for idx, row in df.iterrows()}\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(\"document_metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata_mapping, f)\n",
    "    \n",
    "    print(\"Saved metadata mapping to 'document_metadata.json'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data: {str(e)}\")\n",
    "\n",
    "print(\"Vectorization and embedding creation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5921e",
   "metadata": {},
   "source": [
    "# Agentic Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for the agent framework\n",
    "import asyncio\n",
    "from typing import Dict, List, Any, Optional\n",
    "import json\n",
    "\n",
    "# Import AutoGen components\n",
    "from autogen import AssistantAgent\n",
    "from autogen import UserMessage\n",
    "from autogen import OllamaChatCompletionClient\n",
    "\n",
    "# For vector database operations\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc29f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_config():\n",
    "    \"\"\"\n",
    "    Creates configuration for Ollama LLM client\n",
    "    \n",
    "    Returns:\n",
    "        OllamaChatCompletionClient: Configured Ollama client\n",
    "    \"\"\"\n",
    "    # Create an Ollama client with a suitable model\n",
    "    # You can change the model to any available in your Ollama installation\n",
    "    model_client = OllamaChatCompletionClient(\n",
    "        model=\"qwen2.5-coder:0.5b\",  # Or any other model you have in Ollama\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return model_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_analyzer_agent(model_client):\n",
    "    \"\"\"\n",
    "    Creates an agent that analyzes and extracts intent from user queries\n",
    "    \n",
    "    Args:\n",
    "        model_client: The LLM client for the agent\n",
    "        \n",
    "    Returns:\n",
    "        AssistantAgent: Configured query analyzer agent\n",
    "    \"\"\"\n",
    "    system_message = \"\"\"You are a Query Analyzer Agent specialized in understanding user queries about customer visit data.\n",
    "    Your task is to:\n",
    "    1. Identify the main intent of the query (e.g., customer information, visit outcomes, regional analysis)\n",
    "    2. Extract key entities mentioned in the query (customers, regions, dates, products)\n",
    "    3. Determine time ranges if mentioned (specific dates, months, quarters)\n",
    "    4. Identify any filtering conditions (e.g., specific regions, product divisions)\n",
    "    5. Format the extracted information in a structured way for the retrieval agent\n",
    "    \n",
    "    Respond with a JSON object containing:\n",
    "    - intent: The primary purpose of the query\n",
    "    - entities: Key entities mentioned\n",
    "    - time_range: Any time periods mentioned\n",
    "    - filters: Any filtering conditions\n",
    "    - search_terms: Key terms for vector search\n",
    "    \"\"\"\n",
    "    \n",
    "    return AssistantAgent(\n",
    "        name=\"QueryAnalyzer\",\n",
    "        model_client=model_client,\n",
    "        system_message=system_message\n",
    "    )\n",
    "\n",
    "def create_data_retriever_agent(model_client, collection):\n",
    "    \"\"\"\n",
    "    Creates an agent that retrieves relevant data from the vector database\n",
    "    \n",
    "    Args:\n",
    "        model_client: The LLM client for the agent\n",
    "        collection: ChromaDB collection for vector search\n",
    "        \n",
    "    Returns:\n",
    "        AssistantAgent: Configured data retriever agent\n",
    "    \"\"\"\n",
    "    system_message = \"\"\"You are a Data Retriever Agent specialized in finding relevant customer visit information.\n",
    "    Your task is to:\n",
    "    1. Use the structured query information to search the vector database\n",
    "    2. Apply any filters to narrow down results\n",
    "    3. Retrieve the most relevant entries\n",
    "    4. Format the retrieved data in a clear, structured way\n",
    "    5. Include metadata about the search results (total found, relevance)\n",
    "    \n",
    "    The vector database contains customer visit reports with the following fields:\n",
    "    - Visit dates\n",
    "    - Customer information\n",
    "    - Product divisions\n",
    "    - Regional data\n",
    "    - Visit outcomes\n",
    "    - Next steps\n",
    "    \n",
    "    Respond with a JSON object containing:\n",
    "    - retrieved_data: The relevant entries found\n",
    "    - metadata: Information about the search results\n",
    "    - suggested_focus: Areas that might be most relevant to the query\n",
    "    \"\"\"\n",
    "    \n",
    "    async def search_vector_db(query_info):\n",
    "        \"\"\"Tool to search the vector database based on query information\"\"\"\n",
    "        try:\n",
    "            # Extract search terms from query info\n",
    "            search_terms = query_info.get(\"search_terms\", [])\n",
    "            if not search_terms:\n",
    "                return {\"error\": \"No search terms provided\"}\n",
    "            \n",
    "            # Combine search terms into a query string\n",
    "            query_string = \" \".join(search_terms) if isinstance(search_terms, list) else search_terms\n",
    "            \n",
    "            # Perform the vector search\n",
    "            results = collection.query(\n",
    "                query_texts=[query_string],\n",
    "                n_results=5  # Adjust as needed\n",
    "            )\n",
    "            \n",
    "            # Format the results\n",
    "            formatted_results = []\n",
    "            if results and 'documents' in results and results['documents']:\n",
    "                for i, doc in enumerate(results['documents'][0]):\n",
    "                    # Get metadata if available\n",
    "                    metadata = {}\n",
    "                    if 'metadatas' in results and results['metadatas'] and i < len(results['metadatas'][0]):\n",
    "                        metadata = results['metadatas'][0][i]\n",
    "                    \n",
    "                    # Format the entry\n",
    "                    formatted_results.append({\n",
    "                        \"text\": doc,\n",
    "                        \"metadata\": metadata,\n",
    "                        \"distance\": results['distances'][0][i] if 'distances' in results else None\n",
    "                    })\n",
    "            \n",
    "            return {\n",
    "                \"retrieved_data\": formatted_results,\n",
    "                \"metadata\": {\n",
    "                    \"total_found\": len(formatted_results),\n",
    "                    \"query\": query_string\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error searching vector database: {str(e)}\"}\n",
    "    \n",
    "    retriever = AssistantAgent(\n",
    "        name=\"DataRetriever\",\n",
    "        model_client=model_client,\n",
    "        system_message=system_message,\n",
    "        tools=[search_vector_db]\n",
    "    )\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "def create_response_generator_agent(model_client):\n",
    "    \"\"\"\n",
    "    Creates an agent that generates final responses based on retrieved data\n",
    "    \n",
    "    Args:\n",
    "        model_client: The LLM client for the agent\n",
    "        \n",
    "    Returns:\n",
    "        AssistantAgent: Configured response generator agent\n",
    "    \"\"\"\n",
    "    system_message = \"\"\"You are a Response Generator Agent specialized in creating informative responses about customer visit data.\n",
    "    Your task is to:\n",
    "    1. Use the retrieved data to create a comprehensive response\n",
    "    2. Structure the information in a clear, readable format\n",
    "    3. Highlight key insights and patterns\n",
    "    4. Provide direct answers to the original query\n",
    "    5. Include relevant details from the visit reports\n",
    "    \n",
    "    Make your responses:\n",
    "    - Concise but informative\n",
    "    - Well-structured with headings when appropriate\n",
    "    - Focused on answering the original question\n",
    "    - Supported by data from the retrieved documents\n",
    "    \n",
    "    If the data is insufficient to answer the query completely, acknowledge the limitations.\n",
    "    \"\"\"\n",
    "    \n",
    "    return AssistantAgent(\n",
    "        name=\"ResponseGenerator\",\n",
    "        model_client=model_client,\n",
    "        system_message=system_message\n",
    "    )\n",
    "\n",
    "def create_orchestrator_agent(model_client):\n",
    "    \"\"\"\n",
    "    Creates an agent that orchestrates the workflow between other agents\n",
    "    \n",
    "    Args:\n",
    "        model_client: The LLM client for the agent\n",
    "        \n",
    "    Returns:\n",
    "        AssistantAgent: Configured orchestrator agent\n",
    "    \"\"\"\n",
    "    system_message = \"\"\"You are an Orchestrator Agent responsible for managing the workflow between specialized agents.\n",
    "    Your task is to:\n",
    "    1. Receive user queries and direct them to the Query Analyzer\n",
    "    2. Take the structured query information and pass it to the Data Retriever\n",
    "    3. Send retrieved data to the Response Generator\n",
    "    4. Return the final response to the user\n",
    "    5. Handle any errors or edge cases in the process\n",
    "    \n",
    "    Maintain the logical flow of information between agents and ensure each step is completed properly before proceeding to the next.\n",
    "    \"\"\"\n",
    "    \n",
    "    return AssistantAgent(\n",
    "        name=\"Orchestrator\",\n",
    "        model_client=model_client,\n",
    "        system_message=system_message\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab88d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentWorkflow:\n",
    "    \"\"\"\n",
    "    Manages the workflow between agents for processing user queries\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection):\n",
    "        \"\"\"\n",
    "        Initialize the agent workflow\n",
    "        \n",
    "        Args:\n",
    "            collection: ChromaDB collection for vector search\n",
    "        \"\"\"\n",
    "        # Initialize the LLM client\n",
    "        self.model_client = get_ollama_config()\n",
    "        \n",
    "        # Create the agents\n",
    "        self.query_analyzer = create_query_analyzer_agent(self.model_client)\n",
    "        self.data_retriever = create_data_retriever_agent(self.model_client, collection)\n",
    "        self.response_generator = create_response_generator_agent(self.model_client)\n",
    "        self.orchestrator = create_orchestrator_agent(self.model_client)\n",
    "        \n",
    "        print(\"Agent workflow initialized with all required agents\")\n",
    "    \n",
    "    async def process_query(self, user_query):\n",
    "        \"\"\"\n",
    "        Process a user query through the agent workflow\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's natural language query\n",
    "            \n",
    "        Returns:\n",
    "            str: The final response to the user query\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Processing query: {user_query}\")\n",
    "            \n",
    "            # Step 1: Analyze the query\n",
    "            print(\"Step 1: Analyzing query...\")\n",
    "            analyzer_result = await self.query_analyzer.run(task=user_query)\n",
    "            \n",
    "            # Extract the structured query information\n",
    "            try:\n",
    "                query_info = json.loads(analyzer_result.content)\n",
    "                print(f\"Query analysis complete: {json.dumps(query_info, indent=2)}\")\n",
    "            except json.JSONDecodeError:\n",
    "                # If the response is not valid JSON, use a simplified approach\n",
    "                query_info = {\"search_terms\": user_query}\n",
    "                print(\"Could not parse analyzer response as JSON. Using simplified query info.\")\n",
    "            \n",
    "            # Step 2: Retrieve relevant data\n",
    "            print(\"Step 2: Retrieving data...\")\n",
    "            retriever_message = f\"Search for information related to: {json.dumps(query_info)}\"\n",
    "            retriever_result = await self.data_retriever.run(task=retriever_message)\n",
    "            \n",
    "            # Step 3: Generate the response\n",
    "            print(\"Step 3: Generating response...\")\n",
    "            generator_message = f\"\"\"\n",
    "            Original query: {user_query}\n",
    "            \n",
    "            Retrieved information: {retriever_result.content}\n",
    "            \n",
    "            Please generate a comprehensive response to the original query based on this information.\n",
    "            \"\"\"\n",
    "            response_result = await self.response_generator.run(task=generator_message)\n",
    "            \n",
    "            print(\"Query processing complete\")\n",
    "            return response_result.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_message = f\"Error processing query: {str(e)}\"\n",
    "            print(error_message)\n",
    "            return f\"I encountered an error while processing your query: {str(e)}\"\n",
    "    \n",
    "    async def close(self):\n",
    "        \"\"\"Close the model client connection\"\"\"\n",
    "        await self.model_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4749d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_agent_workflow(collection):\n",
    "    \"\"\"\n",
    "    Set up the agent workflow\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection for vector search\n",
    "        \n",
    "    Returns:\n",
    "        AgentWorkflow: Configured agent workflow\n",
    "    \"\"\"\n",
    "    return AgentWorkflow(collection)\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to run the agent workflow\n",
    "    \"\"\"\n",
    "    # Load the vector database\n",
    "    print(\"Loading vector database...\")\n",
    "    client = chromadb.Client(Settings(persist_directory=\"./chroma_db\"))\n",
    "    collection = client.get_or_create_collection(\"customer_visits\")\n",
    "    \n",
    "    # Set up the agent workflow\n",
    "    print(\"Setting up agent workflow...\")\n",
    "    workflow = setup_agent_workflow(collection)\n",
    "    \n",
    "    print(\"Agent workflow ready!\")\n",
    "    print(\"\\nYou can now ask questions about customer visit data.\")\n",
    "    \n",
    "    # Simple interactive loop for testing\n",
    "    try:\n",
    "        while True:\n",
    "            query = input(\"\\nEnter your query (or 'exit' to quit): \")\n",
    "            if query.lower() == 'exit':\n",
    "                break\n",
    "                \n",
    "            response = await workflow.process_query(query)\n",
    "            print(\"\\nResponse:\")\n",
    "            print(response)\n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        await workflow.close()\n",
    "\n",
    "# Run the main function if this script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_collection(collection):\n",
    "    \"\"\"\n",
    "    Validate that the ChromaDB collection is properly configured\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection to validate\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if valid, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the collection exists and has data\n",
    "        count = collection.count()\n",
    "        if count == 0:\n",
    "            print(\"Warning: Vector database collection is empty\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"Vector database collection contains {count} documents\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating collection: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "async def safe_agent_run(agent, task, max_retries=3):\n",
    "    \"\"\"\n",
    "    Safely run an agent task with retry logic\n",
    "    \n",
    "    Args:\n",
    "        agent: The agent to run\n",
    "        task: The task to run\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        Message: The agent's response\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return await agent.run(task=task)\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"Error running agent task (attempt {retries}/{max_retries}): {str(e)}\")\n",
    "            if retries >= max_retries:\n",
    "                raise\n",
    "            await asyncio.sleep(1)  # Brief pause before retrying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentWorkflow:\n",
    "    \"\"\"\n",
    "    Manages the workflow between agents for processing user queries\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection):\n",
    "        \"\"\"\n",
    "        Initialize the agent workflow\n",
    "        \n",
    "        Args:\n",
    "            collection: ChromaDB collection for vector search\n",
    "        \"\"\"\n",
    "        # Validate the collection\n",
    "        if not validate_collection(collection):\n",
    "            print(\"Warning: Proceeding with potentially invalid collection\")\n",
    "        \n",
    "        # Initialize the LLM client\n",
    "        self.model_client = get_ollama_config()\n",
    "        \n",
    "        # Create the agents\n",
    "        self.query_analyzer = create_query_analyzer_agent(self.model_client)\n",
    "        self.data_retriever = create_data_retriever_agent(self.model_client, collection)\n",
    "        self.response_generator = create_response_generator_agent(self.model_client)\n",
    "        self.orchestrator = create_orchestrator_agent(self.model_client)\n",
    "        \n",
    "        print(\"Agent workflow initialized with all required agents\")\n",
    "    \n",
    "    async def process_query(self, user_query):\n",
    "        \"\"\"\n",
    "        Process a user query through the agent workflow\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's natural language query\n",
    "            \n",
    "        Returns:\n",
    "            str: The final response to the user query\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Processing query: {user_query}\")\n",
    "            \n",
    "            # Step 1: Analyze the query\n",
    "            print(\"Step 1: Analyzing query...\")\n",
    "            analyzer_result = await safe_agent_run(self.query_analyzer, user_query)\n",
    "            \n",
    "            # Extract the structured query information\n",
    "            try:\n",
    "                query_info = json.loads(analyzer_result.content)\n",
    "                print(f\"Query analysis complete: {json.dumps(query_info, indent=2)}\")\n",
    "            except json.JSONDecodeError:\n",
    "                # If the response is not valid JSON, use a simplified approach\n",
    "                query_info = {\"search_terms\": user_query}\n",
    "                print(\"Could not parse analyzer response as JSON. Using simplified query info.\")\n",
    "            \n",
    "            # Step 2: Retrieve relevant data\n",
    "            print(\"Step 2: Retrieving data...\")\n",
    "            retriever_message = f\"Search for information related to: {json.dumps(query_info)}\"\n",
    "            retriever_result = await safe_agent_run(self.data_retriever, retriever_message)\n",
    "            \n",
    "            # Step 3: Generate the response\n",
    "            print(\"Step 3: Generating response...\")\n",
    "            generator_message = f\"\"\"\n",
    "            Original query: {user_query}\n",
    "            \n",
    "            Retrieved information: {retriever_result.content}\n",
    "            \n",
    "            Please generate a comprehensive response to the original query based on this information.\n",
    "            \"\"\"\n",
    "            response_result = await safe_agent_run(self.response_generator, generator_message)\n",
    "            \n",
    "            print(\"Query processing complete\")\n",
    "            return response_result.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_message = f\"Error processing query: {str(e)}\"\n",
    "            print(error_message)\n",
    "            return f\"I encountered an error while processing your query: {str(e)}\"\n",
    "    \n",
    "    async def close(self):\n",
    "        \"\"\"Close the model client connection\"\"\"\n",
    "        await self.model_client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6923d2a5",
   "metadata": {},
   "source": [
    "# Step 6: RAG Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e301169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for RAG implementation\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define the embedding function that's compatible with ChromaDB's interface\n",
    "class SentenceTransformerEmbedding:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding model\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the sentence transformer model to use\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "    def __call__(self, input: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            input: List of strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            List of embeddings as float arrays\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(input)\n",
    "        return embeddings.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d593998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vector_database(df, embedding_function=None):\n",
    "    \"\"\"\n",
    "    Set up and populate the vector database with customer visit data\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing customer visit data\n",
    "        embedding_function: Function to generate embeddings\n",
    "        \n",
    "    Returns:\n",
    "        chromadb.Collection: The populated collection\n",
    "    \"\"\"\n",
    "    print(\"Setting up vector database...\")\n",
    "    \n",
    "    # Initialize ChromaDB client\n",
    "    chroma_path = os.getenv(\"CHROMADB_PATH\", \"./chroma_db\")\n",
    "    client = chromadb.Client(Settings(persist_directory=chroma_path))\n",
    "    \n",
    "    # Create or get the collection\n",
    "    if embedding_function is None:\n",
    "        embedding_function = SentenceTransformerEmbedding()\n",
    "    \n",
    "    # Check if collection exists and recreate if needed\n",
    "    try:\n",
    "        client.delete_collection(\"customer_visits\")\n",
    "        print(\"Existing collection deleted.\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    collection = client.create_collection(\n",
    "        name=\"customer_visits\",\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    \n",
    "    # Prepare documents for insertion\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Create a comprehensive document from the row data\n",
    "        document = f\"\"\"\n",
    "        Visit Date: {row['Visit Plan: Visit Date']}\n",
    "        Created Date: {row['Visit Report: Created Date']}\n",
    "        Region: {row['Visit Plan: Owner Region']}\n",
    "        Owner Email: {row['Visit Plan: Visit Owner Email']}\n",
    "        Owner Name: {row['Visit Plan: Owner Name']}\n",
    "        Customer: {row['Customer']}\n",
    "        Customer SAP Code: {row['Customer SAP Code']}\n",
    "        Product Division: {row['Visit Plan: Product Division']}\n",
    "        Next Steps: {row['Next Steps']}\n",
    "        Outcome of meeting: {row['Outcome of meeting']}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create metadata for filtering\n",
    "        metadata = {\n",
    "            \"visit_date\": str(row['Visit Plan: Visit Date']),\n",
    "            \"created_date\": str(row['Visit Report: Created Date']),\n",
    "            \"region\": row['Visit Plan: Owner Region'],\n",
    "            \"owner_email\": row['Visit Plan: Visit Owner Email'],\n",
    "            \"owner_name\": row['Visit Plan: Owner Name'],\n",
    "            \"customer\": row['Customer'],\n",
    "            \"customer_sap_code\": str(row['Customer SAP Code']),\n",
    "            \"product_division\": row['Visit Plan: Product Division']\n",
    "        }\n",
    "        \n",
    "        documents.append(document)\n",
    "        metadatas.append(metadata)\n",
    "        ids.append(f\"doc_{idx}\")\n",
    "    \n",
    "    # Add documents to the collection in batches\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        end_idx = min(i + batch_size, len(documents))\n",
    "        collection.add(\n",
    "            documents=documents[i:end_idx],\n",
    "            metadatas=metadatas[i:end_idx],\n",
    "            ids=ids[i:end_idx]\n",
    "        )\n",
    "        print(f\"Added documents {i} to {end_idx-1}\")\n",
    "    \n",
    "    print(f\"Vector database setup complete with {len(documents)} documents\")\n",
    "    return collection\n",
    "\n",
    "def load_vector_db():\n",
    "    \"\"\"\n",
    "    Load the existing vector database\n",
    "    \n",
    "    Returns:\n",
    "        chromadb.Collection: The loaded collection\n",
    "    \"\"\"\n",
    "    print(\"Loading vector database...\")\n",
    "    \n",
    "    # Try to get existing collection or create a new one\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(\n",
    "            name=\"customer_visits\",\n",
    "            embedding_function=embedding_function\n",
    "        )\n",
    "        count = collection.count()\n",
    "        print(f\"Loaded existing collection with {count} documents\")\n",
    "    except:\n",
    "        print(\"Collection not found. Please run setup_vector_database first.\")\n",
    "        collection = chroma_client.create_collection(\n",
    "            name=\"customer_visits\",\n",
    "            embedding_function=embedding_function\n",
    "        )\n",
    "    \n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca841074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGProcessor:\n",
    "    \"\"\"\n",
    "    Handles the Retrieval-Augmented Generation process\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection):\n",
    "        \"\"\"\n",
    "        Initialize the RAG processor\n",
    "        \n",
    "        Args:\n",
    "            collection: ChromaDB collection for vector search\n",
    "        \"\"\"\n",
    "        self.collection = collection\n",
    "    \n",
    "    def retrieve(self, query, n_results=5, filter_criteria=None):\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents based on the query\n",
    "        \n",
    "        Args:\n",
    "            query: The query string\n",
    "            n_results: Number of results to retrieve\n",
    "            filter_criteria: Optional filter criteria for the query\n",
    "            \n",
    "        Returns:\n",
    "            dict: Retrieved documents and their metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Perform the vector search\n",
    "            results = self.collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=n_results,\n",
    "                where=filter_criteria\n",
    "            )\n",
    "            \n",
    "            # Format the results\n",
    "            formatted_results = []\n",
    "            if results and 'documents' in results and results['documents']:\n",
    "                for i, doc in enumerate(results['documents'][0]):\n",
    "                    # Get metadata if available\n",
    "                    metadata = {}\n",
    "                    if 'metadatas' in results and results['metadatas'] and i < len(results['metadatas'][0]):\n",
    "                        metadata = results['metadatas'][0][i]\n",
    "                    \n",
    "                    # Format the entry\n",
    "                    formatted_results.append({\n",
    "                        \"text\": doc,\n",
    "                        \"metadata\": metadata,\n",
    "                        \"distance\": results['distances'][0][i] if 'distances' in results else None\n",
    "                    })\n",
    "            \n",
    "            return {\n",
    "                \"retrieved_data\": formatted_results,\n",
    "                \"metadata\": {\n",
    "                    \"total_found\": len(formatted_results),\n",
    "                    \"query\": query\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in retrieval: {str(e)}\")\n",
    "            return {\n",
    "                \"error\": f\"Error retrieving data: {str(e)}\",\n",
    "                \"retrieved_data\": [],\n",
    "                \"metadata\": {\n",
    "                    \"total_found\": 0,\n",
    "                    \"query\": query\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    def augment_context(self, query, retrieved_data):\n",
    "        \"\"\"\n",
    "        Augment the query with context from retrieved documents\n",
    "        \n",
    "        Args:\n",
    "            query: The original query\n",
    "            retrieved_data: Data retrieved from the vector database\n",
    "            \n",
    "        Returns:\n",
    "            str: Augmented context for the LLM\n",
    "        \"\"\"\n",
    "        if not retrieved_data or \"error\" in retrieved_data:\n",
    "            return f\"Query: {query}\\n\\nNo relevant information found in the database.\"\n",
    "        \n",
    "        # Extract the retrieved documents\n",
    "        documents = retrieved_data.get(\"retrieved_data\", [])\n",
    "        if not documents:\n",
    "            return f\"Query: {query}\\n\\nNo relevant information found in the database.\"\n",
    "        \n",
    "        # Build the context\n",
    "        context = f\"Query: {query}\\n\\nRelevant information from customer visit database:\\n\\n\"\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            context += f\"Document {i+1}:\\n{doc['text']}\\n\\n\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    async def generate_response(self, augmented_context, model_client):\n",
    "        \"\"\"\n",
    "        Generate a response using the LLM with augmented context\n",
    "        \n",
    "        Args:\n",
    "            augmented_context: The context-augmented query\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            task_result = await self.response_agent.run(task=augmented_context)\n",
    "\n",
    "            # Preferred: use last_message if available\n",
    "            if hasattr(task_result, \"last_message\"):\n",
    "                return task_result.last_message.content\n",
    "            \n",
    "            # Fallback: pull the last TextMessage from .messages\n",
    "            if hasattr(task_result, \"messages\") and task_result.messages:\n",
    "                # assume the last message is the assistants answer\n",
    "                return task_result.messages[-1].content\n",
    "\n",
    "            # In the unlikely case neither attribute exists\n",
    "            return str(task_result)\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92d603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_enabled_data_retriever_agent(model_client, rag_processor):\n",
    "    \"\"\"\n",
    "    Creates a RAG-enabled agent that retrieves relevant data\n",
    "    \n",
    "    Args:\n",
    "        model_client: The LLM client for the agent\n",
    "        rag_processor: The RAG processor instance\n",
    "        \n",
    "    Returns:\n",
    "        AssistantAgent: Configured RAG-enabled data retriever agent\n",
    "    \"\"\"\n",
    "    system_message = \"\"\"You are a Data Retriever Agent specialized in finding relevant customer visit information.\n",
    "    Your task is to:\n",
    "    1. Analyze the query to identify key search terms\n",
    "    2. Retrieve relevant information from the customer visit database\n",
    "    3. Extract the most pertinent details from the retrieved documents\n",
    "    4. Format the information in a clear, structured way\n",
    "    \n",
    "    The customer visit database contains reports with the following fields:\n",
    "    - Visit dates\n",
    "    - Customer information\n",
    "    - Product divisions\n",
    "    - Regional data\n",
    "    - Visit outcomes\n",
    "    - Next steps\n",
    "    \n",
    "    Respond with a comprehensive set of relevant information that addresses the query.\n",
    "    \"\"\"\n",
    "    \n",
    "    async def retrieve_data(query_info):\n",
    "        \"\"\"Tool to retrieve data from the vector database\"\"\"\n",
    "        try:\n",
    "            # Extract search terms and filters\n",
    "            search_terms = query_info.get(\"search_terms\", \"\")\n",
    "            if not search_terms:\n",
    "                return {\"error\": \"No search terms provided\"}\n",
    "            \n",
    "            # Extract any filter criteria\n",
    "            filters = query_info.get(\"filters\", {})\n",
    "            \n",
    "            # Convert filters to ChromaDB format if needed\n",
    "            chroma_filters = {}\n",
    "            if filters:\n",
    "                for key, value in filters.items():\n",
    "                    if key in [\"region\", \"customer\", \"product_division\", \"owner_name\"]:\n",
    "                        chroma_filters[key] = value\n",
    "            \n",
    "            # Use RAG processor to retrieve relevant data\n",
    "            results = rag_processor.retrieve(\n",
    "                query=search_terms,\n",
    "                n_results=5,\n",
    "                filter_criteria=chroma_filters if chroma_filters else None\n",
    "            )\n",
    "            \n",
    "            # Augment the context with retrieved data\n",
    "            augmented_context = rag_processor.augment_context(search_terms, results)\n",
    "            \n",
    "            return {\n",
    "                \"augmented_context\": augmented_context,\n",
    "                \"retrieved_data\": results.get(\"retrieved_data\", []),\n",
    "                \"metadata\": results.get(\"metadata\", {})\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error retrieving data: {str(e)}\"}\n",
    "    \n",
    "    retriever = AssistantAgent(\n",
    "        name=\"DataRetriever\",\n",
    "        model_client=model_client,\n",
    "        system_message=system_message,\n",
    "        tools=[retrieve_data]\n",
    "    )\n",
    "    \n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEnabledWorkflow:\n",
    "    \"\"\"\n",
    "    Manages the RAG-enabled workflow between agents for processing user queries\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection):\n",
    "        \"\"\"\n",
    "        Initialize the RAG-enabled agent workflow\n",
    "        \n",
    "        Args:\n",
    "            collection: ChromaDB collection for vector search\n",
    "        \"\"\"\n",
    "        # Validate the collection\n",
    "        if not validate_collection(collection):\n",
    "            print(\"Warning: Proceeding with potentially invalid collection\")\n",
    "        \n",
    "        # Initialize the LLM client\n",
    "        self.model_client = get_ollama_config()\n",
    "        \n",
    "        # Initialize the RAG processor\n",
    "        self.rag_processor = RAGProcessor(collection)\n",
    "        \n",
    "        # Create the agents\n",
    "        self.query_analyzer = create_query_analyzer_agent(self.model_client)\n",
    "        self.data_retriever = create_rag_enabled_data_retriever_agent(self.model_client, self.rag_processor)\n",
    "        self.response_generator = create_response_generator_agent(self.model_client)\n",
    "        self.orchestrator = create_orchestrator_agent(self.model_client)\n",
    "        \n",
    "        print(\"RAG-enabled agent workflow initialized with all required agents\")\n",
    "    \n",
    "    async def process_query(self, user_query):\n",
    "        \"\"\"\n",
    "        Process a user query through the RAG-enabled agent workflow\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's natural language query\n",
    "            \n",
    "        Returns:\n",
    "            str: The final response to the user query\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Processing query: {user_query}\")\n",
    "            \n",
    "            # Step 1: Analyze the query\n",
    "            print(\"Step 1: Analyzing query...\")\n",
    "            analyzer_result = await safe_agent_run(self.query_analyzer, user_query)\n",
    "            \n",
    "            # Extract the structured query information\n",
    "            try:\n",
    "                query_info = json.loads(analyzer_result.content)\n",
    "                print(f\"Query analysis complete: {json.dumps(query_info, indent=2)}\")\n",
    "            except json.JSONDecodeError:\n",
    "                # If the response is not valid JSON, use a simplified approach\n",
    "                query_info = {\"search_terms\": user_query}\n",
    "                print(\"Could not parse analyzer response as JSON. Using simplified query info.\")\n",
    "            \n",
    "            # Step 2: Retrieve relevant data with RAG\n",
    "            print(\"Step 2: Retrieving data with RAG...\")\n",
    "            retriever_message = f\"Search for information related to: {json.dumps(query_info)}\"\n",
    "            retriever_result = await safe_agent_run(self.data_retriever, retriever_message)\n",
    "            \n",
    "            # Step 3: Generate the response\n",
    "            print(\"Step 3: Generating response...\")\n",
    "            generator_message = f\"\"\"\n",
    "            Original query: {user_query}\n",
    "            \n",
    "            Retrieved information: {retriever_result.content}\n",
    "            \n",
    "            Please generate a comprehensive response to the original query based on this information.\n",
    "            \"\"\"\n",
    "            response_result = await safe_agent_run(self.response_generator, generator_message)\n",
    "            \n",
    "            print(\"Query processing complete\")\n",
    "            return response_result.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_message = f\"Error processing query: {str(e)}\"\n",
    "            print(error_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d039d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rag_workflow(collection):\n",
    "    \"\"\"\n",
    "    Set up the RAG-enabled agent workflow\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection for vector search\n",
    "        \n",
    "    Returns:\n",
    "        RAGEnabledWorkflow: Configured RAG-enabled agent workflow\n",
    "    \"\"\"\n",
    "    return RAGEnabledWorkflow(collection)\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to run the RAG-enabled agent workflow\n",
    "    \"\"\"\n",
    "    # Load the vector database\n",
    "    print(\"Loading vector database...\")\n",
    "    collection = load_vector_db()\n",
    "    \n",
    "    # Set up the RAG-enabled agent workflow\n",
    "    print(\"Setting up RAG-enabled agent workflow...\")\n",
    "    workflow = setup_rag_workflow(collection)\n",
    "    \n",
    "    print(\"RAG-enabled agent workflow ready!\")\n",
    "    print(\"\\nYou can now ask questions about customer visit data.\")\n",
    "    \n",
    "    # Simple interactive loop for testing\n",
    "    try:\n",
    "        while True:\n",
    "            query = input(\"\\nEnter your query (or 'exit' to quit): \")\n",
    "            if query.lower() == 'exit':\n",
    "                break\n",
    "                \n",
    "            response = await workflow.process_query(query)\n",
    "            print(\"\\nResponse:\")\n",
    "            print(response)\n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        await workflow.close()\n",
    "\n",
    "# Run the main function if this script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2114bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_context_with_metadata(retrieved_documents):\n",
    "    \"\"\"\n",
    "    Enhance the context with metadata analysis\n",
    "    \n",
    "    Args:\n",
    "        retrieved_documents: List of retrieved documents\n",
    "        \n",
    "    Returns:\n",
    "        str: Enhanced context with metadata insights\n",
    "    \"\"\"\n",
    "    # Extract metadata from documents\n",
    "    regions = []\n",
    "    customers = []\n",
    "    product_divisions = []\n",
    "    dates = []\n",
    "    \n",
    "    for doc in retrieved_documents:\n",
    "        metadata = doc.get(\"metadata\", {})\n",
    "        if \"region\" in metadata and metadata[\"region\"]:\n",
    "            regions.append(metadata[\"region\"])\n",
    "        if \"customer\" in metadata and metadata[\"customer\"]:\n",
    "            customers.append(metadata[\"customer\"])\n",
    "        if \"product_division\" in metadata and metadata[\"product_division\"]:\n",
    "            product_divisions.append(metadata[\"product_division\"])\n",
    "        if \"visit_date\" in metadata and metadata[\"visit_date\"]:\n",
    "            dates.append(metadata[\"visit_date\"])\n",
    "    \n",
    "    # Generate insights from metadata\n",
    "    insights = []\n",
    "    \n",
    "    # Region insights\n",
    "    if regions:\n",
    "        region_counts = {}\n",
    "        for region in regions:\n",
    "            region_counts[region] = region_counts.get(region, 0) + 1\n",
    "        most_common_region = max(region_counts.items(), key=lambda x: x[1])[0]\n",
    "        insights.append(f\"Most relevant region: {most_common_region}\")\n",
    "    \n",
    "    # Customer insights\n",
    "    if customers:\n",
    "        customer_counts = {}\n",
    "        for customer in customers:\n",
    "            customer_counts[customer] = customer_counts.get(customer, 0) + 1\n",
    "        most_common_customers = sorted(customer_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        if most_common_customers:\n",
    "            insights.append(f\"Most relevant customers: {', '.join([c[0] for c in most_common_customers])}\")\n",
    "    \n",
    "    # Product division insights\n",
    "    if product_divisions:\n",
    "        division_counts = {}\n",
    "        for division in product_divisions:\n",
    "            division_counts[division] = division_counts.get(division, 0) + 1\n",
    "        most_common_divisions = sorted(division_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        if most_common_divisions:\n",
    "            insights.append(f\"Most relevant product divisions: {', '.join([d[0] for d in most_common_divisions])}\")\n",
    "    \n",
    "    # Date insights\n",
    "    if dates:\n",
    "        try:\n",
    "            parsed_dates = [dateparser.parse(date) for date in dates if date]\n",
    "            valid_dates = [date for date in parsed_dates if date]\n",
    "            if valid_dates:\n",
    "                earliest_date = min(valid_dates)\n",
    "                latest_date = max(valid_dates)\n",
    "                insights.append(f\"Date range: {earliest_date.strftime('%Y-%m-%d')} to {latest_date.strftime('%Y-%m-%d')}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Combine insights\n",
    "    if insights:\n",
    "        return \"\\n\".join(insights)\n",
    "    else:\n",
    "        return \"No additional insights available from metadata.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424bd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_context(self, query, retrieved_data):\n",
    "    \"\"\"\n",
    "    Augment the query with context from retrieved documents\n",
    "    \n",
    "    Args:\n",
    "        query: The original query\n",
    "        retrieved_data: Data retrieved from the vector database\n",
    "        \n",
    "    Returns:\n",
    "        str: Augmented context for the LLM\n",
    "    \"\"\"\n",
    "    if not retrieved_data or \"error\" in retrieved_data:\n",
    "        return f\"Query: {query}\\n\\nNo relevant information found in the database.\"\n",
    "    \n",
    "    # Extract the retrieved documents\n",
    "    documents = retrieved_data.get(\"retrieved_data\", [])\n",
    "    if not documents:\n",
    "        return f\"Query: {query}\\n\\nNo relevant information found in the database.\"\n",
    "    \n",
    "    # Build the context\n",
    "    context = f\"Query: {query}\\n\\n\"\n",
    "    \n",
    "    # Add metadata insights\n",
    "    metadata_insights = enhance_context_with_metadata(documents)\n",
    "    context += f\"Insights from the data:\\n{metadata_insights}\\n\\n\"\n",
    "    \n",
    "    # Add the retrieved documents\n",
    "    context += \"Relevant information from customer visit database:\\n\\n\"\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        context += f\"Document {i+1}:\\n{doc['text']}\\n\\n\"\n",
    "    \n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00521e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def initialize_system(df=None):\n",
    "    \"\"\"\n",
    "    Initialize the entire system\n",
    "    \n",
    "    Args:\n",
    "        df: Optional DataFrame with customer visit data\n",
    "        \n",
    "    Returns:\n",
    "        RAGEnabledWorkflow: The initialized workflow\n",
    "    \"\"\"\n",
    "    # Set up the vector database if DataFrame is provided\n",
    "    if df is not None:\n",
    "        collection = setup_vector_database(df)\n",
    "    else:\n",
    "        # Otherwise, load the existing database\n",
    "        collection = load_vector_db()\n",
    "    \n",
    "    # Set up the RAG-enabled workflow\n",
    "    workflow = setup_rag_workflow(collection)\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "async def process_user_query(workflow, query):\n",
    "    \"\"\"\n",
    "    Process a user query and return the response\n",
    "    \n",
    "    Args:\n",
    "        workflow: The RAG-enabled workflow\n",
    "        query: The user query\n",
    "        \n",
    "    Returns:\n",
    "        str: The response to the query\n",
    "    \"\"\"\n",
    "    return await workflow.process_query(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2805c3f",
   "metadata": {},
   "source": [
    "# Query Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e4a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for the query processing pipeline\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import dateparser\n",
    "\n",
    "# For embedding and vector search\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# For NLP processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# For AutoGen integration\n",
    "from autogen import AssistantAgent\n",
    "from autogen import UserMessage\n",
    "from autogen import OllamaChatCompletionClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryProcessor:\n",
    "    \"\"\"\n",
    "    Processes user queries to extract temporal references and key terms\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model=None):\n",
    "        \"\"\"\n",
    "        Initialize the query processor\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: SentenceTransformer model for embedding generation\n",
    "        \"\"\"\n",
    "        # Initialize NLP components\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Initialize embedding model if not provided\n",
    "        if embedding_model is None:\n",
    "            self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        else:\n",
    "            self.embedding_model = embedding_model\n",
    "        \n",
    "        # Temporal reference patterns\n",
    "        self.temporal_patterns = {\n",
    "            'date': r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b|\\b\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\\b',\n",
    "            'month_year': r'\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)[,\\s]+\\d{4}\\b',\n",
    "            'month': r'\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\b',\n",
    "            'year': r'\\b\\d{4}\\b',\n",
    "            'quarter': r'\\bQ[1-4]\\s+\\d{4}\\b|\\b(?:first|second|third|fourth)\\s+quarter\\b',\n",
    "            'relative': r'\\blast\\s+(?:week|month|year|quarter)\\b|\\bnext\\s+(?:week|month|year|quarter)\\b|\\bprevious\\s+(?:week|month|year|quarter)\\b|\\bcurrent\\s+(?:week|month|year|quarter)\\b'\n",
    "        }\n",
    "    \n",
    "    def extract_temporal_references(self, query):\n",
    "        \"\"\"\n",
    "        Extract temporal references from the query\n",
    "        \n",
    "        Args:\n",
    "            query: The user query\n",
    "            \n",
    "        Returns:\n",
    "            dict: Extracted temporal references\n",
    "        \"\"\"\n",
    "        temporal_refs = {}\n",
    "        \n",
    "        # Extract dates and time periods\n",
    "        for ref_type, pattern in self.temporal_patterns.items():\n",
    "            matches = re.findall(pattern, query, re.IGNORECASE)\n",
    "            if matches:\n",
    "                temporal_refs[ref_type] = matches\n",
    "        \n",
    "        # Try to parse any dates found\n",
    "        parsed_dates = []\n",
    "        current_date = datetime.now()\n",
    "        \n",
    "        # Process explicit dates\n",
    "        if 'date' in temporal_refs:\n",
    "            for date_str in temporal_refs['date']:\n",
    "                try:\n",
    "                    parsed_date = dateparser.parse(date_str)\n",
    "                    if parsed_date:\n",
    "                        parsed_dates.append(parsed_date)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Process month and year combinations\n",
    "        if 'month_year' in temporal_refs:\n",
    "            for month_year in temporal_refs['month_year']:\n",
    "                try:\n",
    "                    parsed_date = dateparser.parse(month_year)\n",
    "                    if parsed_date:\n",
    "                        parsed_dates.append(parsed_date)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Process relative time references\n",
    "        if 'relative' in temporal_refs:\n",
    "            for rel_time in temporal_refs['relative']:\n",
    "                try:\n",
    "                    parsed_date = dateparser.parse(rel_time, settings={'RELATIVE_BASE': current_date})\n",
    "                    if parsed_date:\n",
    "                        parsed_dates.append(parsed_date)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Add parsed dates to the result\n",
    "        if parsed_dates:\n",
    "            temporal_refs['parsed_dates'] = [date.strftime('%Y-%m-%d') for date in parsed_dates]\n",
    "        \n",
    "        return temporal_refs\n",
    "    \n",
    "    def extract_key_terms(self, query):\n",
    "        \"\"\"\n",
    "        Extract key terms from the query\n",
    "        \n",
    "        Args:\n",
    "            query: The user query\n",
    "            \n",
    "        Returns:\n",
    "            list: Extracted key terms\n",
    "        \"\"\"\n",
    "        # Tokenize the query\n",
    "        tokens = word_tokenize(query.lower())\n",
    "        \n",
    "        # Remove stopwords and punctuation\n",
    "        filtered_tokens = [self.lemmatizer.lemmatize(token) for token in tokens \n",
    "                          if token.isalnum() and token not in self.stop_words]\n",
    "        \n",
    "        # Extract potential entity names (capitalized words)\n",
    "        potential_entities = []\n",
    "        for match in re.finditer(r'\\b[A-Z][a-zA-Z]*(?:\\s+[A-Z][a-zA-Z]*)*\\b', query):\n",
    "            potential_entities.append(match.group())\n",
    "        \n",
    "        # Combine filtered tokens and potential entities\n",
    "        key_terms = list(set(filtered_tokens + potential_entities))\n",
    "        \n",
    "        return key_terms\n",
    "    \n",
    "    def vectorize_query(self, query):\n",
    "        \"\"\"\n",
    "        Generate embedding vector for the query\n",
    "        \n",
    "        Args:\n",
    "            query: The user query\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Query embedding vector\n",
    "        \"\"\"\n",
    "        return self.embedding_model.encode(query)\n",
    "    \n",
    "    def process_query(self, query):\n",
    "        \"\"\"\n",
    "        Process the user query to extract information and generate embedding\n",
    "        \n",
    "        Args:\n",
    "            query: The user query\n",
    "            \n",
    "        Returns:\n",
    "            dict: Processed query information\n",
    "        \"\"\"\n",
    "        # Extract temporal references\n",
    "        temporal_refs = self.extract_temporal_references(query)\n",
    "        \n",
    "        # Extract key terms\n",
    "        key_terms = self.extract_key_terms(query)\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.vectorize_query(query)\n",
    "        \n",
    "        # Combine all information\n",
    "        processed_query = {\n",
    "            \"original_query\": query,\n",
    "            \"temporal_references\": temporal_refs,\n",
    "            \"key_terms\": key_terms,\n",
    "            \"embedding\": query_embedding\n",
    "        }\n",
    "        \n",
    "        return processed_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryPipeline:\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for processing user queries and generating responses\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection, embedding_model=None):\n",
    "        \"\"\"\n",
    "        Initialize the query pipeline\n",
    "        \n",
    "        Args:\n",
    "            collection: ChromaDB collection for vector search\n",
    "            embedding_model: SentenceTransformer model for embedding generation\n",
    "        \"\"\"\n",
    "        # Initialize embedding model\n",
    "        if embedding_model is None:\n",
    "            self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        else:\n",
    "            self.embedding_model = embedding_model\n",
    "        \n",
    "        # Initialize query processor\n",
    "        self.query_processor = QueryProcessor(embedding_model=self.embedding_model)\n",
    "        \n",
    "        # Store collection\n",
    "        self.collection = collection\n",
    "        \n",
    "        # Initialize Ollama client for LLM\n",
    "        self.model_client = OllamaChatCompletionClient(\n",
    "            model=\"qwen2.5-coder:0.5b\",  # Or any other model you have in Ollama\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Create the response generator agent\n",
    "        self.response_agent = self.create_response_generator_agent()\n",
    "        \n",
    "    async def generate_response(self, augmented_context):\n",
    "        \"\"\"\n",
    "        Generate a response using the LLM with augmented context\n",
    "        \n",
    "        Args:\n",
    "            augmented_context: The context-augmented query\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            task_result = await self.response_agent.run(task=augmented_context)\n",
    "\n",
    "            # Preferred: use last_message if available\n",
    "            if hasattr(task_result, \"last_message\"):\n",
    "                return task_result.last_message.content\n",
    "            \n",
    "            # Fallback: pull the last TextMessage from .messages\n",
    "            if hasattr(task_result, \"messages\") and task_result.messages:\n",
    "                # assume the last message is the assistants answer\n",
    "                return task_result.messages[-1].content\n",
    "\n",
    "            # In the unlikely case neither attribute exists\n",
    "            return str(task_result)\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "    \n",
    "    def create_response_generator_agent(self):\n",
    "        \"\"\"\n",
    "        Create the response generator agent\n",
    "        \n",
    "        Returns:\n",
    "            AssistantAgent: The response generator agent\n",
    "        \"\"\"\n",
    "        system_message = \"\"\"You are a Response Generator Agent specialized in creating informative responses about customer visit data.\n",
    "        Your task is to:\n",
    "        1. Use the retrieved data to create a comprehensive response\n",
    "        2. Structure the information in a clear, readable format\n",
    "        3. Highlight key insights and patterns\n",
    "        4. Provide direct answers to the original query\n",
    "        5. Include relevant details from the visit reports\n",
    "        \n",
    "        Make your responses:\n",
    "        - Concise but informative\n",
    "        - Well-structured with headings when appropriate\n",
    "        - Focused on answering the original question\n",
    "        - Supported by data from the retrieved documents\n",
    "        \n",
    "        If the data is insufficient to answer the query completely, acknowledge the limitations.\n",
    "        \"\"\"\n",
    "        \n",
    "        return AssistantAgent(\n",
    "            name=\"ResponseGenerator\",\n",
    "            model_client=self.model_client,\n",
    "            system_message=system_message\n",
    "        )\n",
    "    \n",
    "    async def search_vector_db(self, processed_query, n_results=5):\n",
    "        \"\"\"\n",
    "        Search the vector database using the processed query\n",
    "        \n",
    "        Args:\n",
    "            processed_query: The processed query information\n",
    "            n_results: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            dict: Search results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract query embedding\n",
    "            query_embedding = processed_query[\"embedding\"].tolist()\n",
    "            \n",
    "            # Extract temporal references for filtering\n",
    "            temporal_refs = processed_query[\"temporal_references\"]\n",
    "            \n",
    "            # Prepare filter criteria if temporal references exist\n",
    "            filter_criteria = None\n",
    "            if temporal_refs and \"parsed_dates\" in temporal_refs:\n",
    "                # This is a simplified example - you would need to adapt this\n",
    "                # to your specific database schema and filtering capabilities\n",
    "                pass\n",
    "            \n",
    "            # Perform the vector search\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=n_results,\n",
    "                where=filter_criteria\n",
    "            )\n",
    "            \n",
    "            docs   = results[\"documents\"][0]\n",
    "            metas  = results.get(\"metadatas\", [[]])[0]\n",
    "            ids    = results.get(\"ids\", [[]])[0]\n",
    "            dists  = results.get(\"distances\", [[]])[0]\n",
    "            \n",
    "            # Format the results\n",
    "            formatted_results = []\n",
    "            if results and 'documents' in results and results['documents']:\n",
    "                for doc, meta, _id, dist in zip(docs, metas, ids, dists):\n",
    "                    # Get metadata if available\n",
    "                    metadata = {}\n",
    "                    if 'metadatas' in results and results['metadatas'] and i < len(results['metadatas'][0]):\n",
    "                        metadata = results['metadatas'][0][i]\n",
    "                    \n",
    "                    # Format the entry\n",
    "                    formatted_results.append({\n",
    "                        \"text\": doc,\n",
    "                        \"metadata\": metadata,\n",
    "                        \"distance\": results['distances'][0][i] if 'distances' in results else None,\n",
    "                        \"id\": _id,\n",
    "                        \"text\": doc,\n",
    "                        \"distance\": dist,\n",
    "                        \"metadata\": metadata   \n",
    "                    })\n",
    "            \n",
    "            return {\n",
    "                \"retrieved_data\": formatted_results,\n",
    "                \"metadata\": {\n",
    "                    \"total_found\": len(formatted_results),\n",
    "                    \"query\": processed_query[\"original_query\"]\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in vector search: {str(e)}\")\n",
    "            return {\n",
    "                \"error\": f\"Error retrieving data: {str(e)}\",\n",
    "                \"retrieved_data\": [],\n",
    "                \"metadata\": {\n",
    "                    \"total_found\": 0,\n",
    "                    \"query\": processed_query[\"original_query\"]\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    def augment_context(self, processed_query, search_results):\n",
    "        \"\"\"\n",
    "        Augment the context with search results and query information\n",
    "        \n",
    "        Args:\n",
    "            processed_query: The processed query information\n",
    "            search_results: The search results from vector database\n",
    "            \n",
    "        Returns:\n",
    "            str: Augmented context\n",
    "        \"\"\"\n",
    "        if not search_results or \"error\" in search_results:\n",
    "            return f\"Query: {processed_query['original_query']}\\n\\nNo relevant information found in the database.\"\n",
    "        \n",
    "        # Extract the retrieved documents\n",
    "        documents = search_results.get(\"retrieved_data\", [])\n",
    "        if not documents:\n",
    "            return f\"Query: {processed_query['original_query']}\\n\\nNo relevant information found in the database.\"\n",
    "        \n",
    "        # Build the context\n",
    "        context = f\"Query: {processed_query['original_query']}\\n\\n\"\n",
    "        \n",
    "        # Add temporal references if available\n",
    "        temporal_refs = processed_query.get(\"temporal_references\", {})\n",
    "        if temporal_refs:\n",
    "            context += \"Temporal References in Query:\\n\"\n",
    "            for ref_type, refs in temporal_refs.items():\n",
    "                if ref_type != \"parsed_dates\":\n",
    "                    context += f\"- {ref_type}: {', '.join(refs)}\\n\"\n",
    "            if \"parsed_dates\" in temporal_refs:\n",
    "                context += f\"- Parsed Dates: {', '.join(temporal_refs['parsed_dates'])}\\n\"\n",
    "            context += \"\\n\"\n",
    "        \n",
    "        # Add key terms if available\n",
    "        key_terms = processed_query.get(\"key_terms\", [])\n",
    "        if key_terms:\n",
    "            context += f\"Key Terms in Query: {', '.join(key_terms)}\\n\\n\"\n",
    "        \n",
    "        # Add the retrieved documents\n",
    "        context += \"Relevant information from customer visit database:\\n\\n\"\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            context += f\"Document {i+1}:\\n{doc['text']}\\n\\n\"\n",
    "            if \"metadata\" in doc and doc[\"metadata\"]:\n",
    "                context += \"Metadata:\\n\"\n",
    "                for key, value in doc[\"metadata\"].items():\n",
    "                    context += f\"- {key}: {value}\\n\"\n",
    "                context += \"\\n\"\n",
    "        \n",
    "        return context\n",
    "\n",
    "    \n",
    "    async def process(self, query):\n",
    "        \"\"\"\n",
    "        Process a user query through the entire pipeline\n",
    "        \n",
    "        Args:\n",
    "            query: The user query\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Processing query: {query}\")\n",
    "            \n",
    "            # Step 1: Process the query\n",
    "            print(\"Step 1: Processing query...\")\n",
    "            processed_query = self.query_processor.process_query(query)\n",
    "            \n",
    "            # Step 2: Search the vector database\n",
    "            print(\"Step 2: Searching vector database...\")\n",
    "            search_results = await self.search_vector_db(processed_query)\n",
    "                       \n",
    "            try:            \n",
    "                # DEBUG: print what we got\n",
    "                print(\"\\nRetrieved hits:\")\n",
    "                for hit in search_results[\"retrieved_data\"]:  \n",
    "                    print(f\"  ID={hit['id']}, dist={hit['distance']:.4f}, customer={hit['metadata']['customer']}\")\n",
    "                    print(f\"    {hit['text'][:100]}\\n\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Step 3: Augment the context\n",
    "            print(\"Step 3: Augmenting context...\")\n",
    "            augmented_context = self.augment_context(processed_query, search_results)\n",
    "            \n",
    "            # Step 4: Generate the response\n",
    "            print(\"Step 4: Generating response...\")\n",
    "            response = await self.generate_response(augmented_context)\n",
    "            \n",
    "            print(\"Query processing complete\")\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_message = f\"Error processing query: {str(e)}\"\n",
    "            print(error_message)\n",
    "            return f\"I encountered an error while processing your query: {str(e)}\"\n",
    "    \n",
    "    async def close(self):\n",
    "        \"\"\"Close the model client connection\"\"\"\n",
    "        await self.model_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc3b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_query_pipeline(collection):\n",
    "    \"\"\"\n",
    "    Set up the query processing pipeline\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection for vector search\n",
    "        \n",
    "    Returns:\n",
    "        QueryPipeline: Configured query pipeline\n",
    "    \"\"\"\n",
    "    return QueryPipeline(collection)\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to run the query processing pipeline\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(\"customer_visits.csv\")\n",
    "    print(f\"Dataset loaded with {len(df)} rows\")\n",
    "    \n",
    "    # Set up the embedding model\n",
    "    print(\"Loading embedding model...\")\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Set up the vector database\n",
    "    print(\"Setting up vector database...\")\n",
    "    chroma_path = os.getenv(\"CHROMADB_PATH\", \"./chroma_db\")\n",
    "    client = chromadb.Client(Settings(persist_directory=chroma_path))\n",
    "    \n",
    "    # Create embedding function for ChromaDB\n",
    "    class SentenceTransformerEmbedding:\n",
    "        def __init__(self, model):\n",
    "            self.model = model\n",
    "            \n",
    "        def __call__(self, input):\n",
    "            embeddings = self.model.encode(input)\n",
    "            return embeddings.tolist()\n",
    "    \n",
    "    embedding_function = SentenceTransformerEmbedding(embedding_model)\n",
    "    \n",
    "    # Check if collection exists, otherwise create and populate it\n",
    "    try:\n",
    "        collection = client.get_collection(\n",
    "            name=\"customer_visits\",\n",
    "            embedding_function=embedding_function\n",
    "        )\n",
    "        print(f\"Loaded existing collection with {collection.count()} documents\")\n",
    "    except:\n",
    "        print(\"Collection not found. Creating and populating...\")\n",
    "        collection = client.create_collection(\n",
    "            name=\"customer_visits\",\n",
    "            embedding_function=embedding_function\n",
    "        )\n",
    "        \n",
    "        # Prepare documents for insertion\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        ids = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # Create a comprehensive document from the row data\n",
    "            document = f\"\"\"\n",
    "            Visit Date: {row['Visit Plan: Visit Date']}\n",
    "            Created Date: {row['Visit Report: Created Date']}\n",
    "            Region: {row['Visit Plan: Owner Region']}\n",
    "            Owner Email: {row['Visit Plan: Visit Owner Email']}\n",
    "            Owner Name: {row['Visit Plan: Owner Name']}\n",
    "            Customer: {row['Customer']}\n",
    "            Customer SAP Code: {row['Customer SAP Code']}\n",
    "            Product Division: {row['Visit Plan: Product Division']}\n",
    "            Next Steps: {row['Next Steps']}\n",
    "            Outcome of meeting: {row['Outcome of meeting']}\n",
    "            \"\"\"\n",
    "            \n",
    "            # Create metadata for filtering\n",
    "            metadata = {\n",
    "                \"visit_date\": str(row['Visit Plan: Visit Date']),\n",
    "                \"created_date\": str(row['Visit Report: Created Date']),\n",
    "                \"region\": row['Visit Plan: Owner Region'],\n",
    "                \"owner_email\": row['Visit Plan: Visit Owner Email'],\n",
    "                \"owner_name\": row['Visit Plan: Owner Name'],\n",
    "                \"customer\": row['Customer'],\n",
    "                \"customer_sap_code\": str(row['Customer SAP Code']),\n",
    "                \"product_division\": row['Visit Plan: Product Division']\n",
    "            }\n",
    "            \n",
    "            documents.append(document)\n",
    "            metadatas.append(metadata)\n",
    "            ids.append(f\"doc_{idx}\")\n",
    "        \n",
    "        # Add documents to the collection in batches\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            end_idx = min(i + batch_size, len(documents))\n",
    "            collection.add(\n",
    "                documents=documents[i:end_idx],\n",
    "                metadatas=metadatas[i:end_idx],\n",
    "                ids=ids[i:end_idx]\n",
    "            )\n",
    "            print(f\"Added documents {i} to {end_idx-1}\")\n",
    "        \n",
    "        print(f\"Vector database setup complete with {len(documents)} documents\")\n",
    "    \n",
    "    # Set up the query pipeline\n",
    "    print(\"Setting up query pipeline...\")\n",
    "    pipeline = setup_query_pipeline(collection)\n",
    "    \n",
    "    print(\"Query pipeline ready!\")\n",
    "    print(\"\\nYou can now ask questions about customer visit data.\")\n",
    "    \n",
    "    # Simple interactive loop for testing\n",
    "    try:\n",
    "        while True:\n",
    "            query = input(\"\\nEnter your query (or 'exit' to quit): \")\n",
    "            if query.lower() == 'exit':\n",
    "                break\n",
    "                \n",
    "            response = await pipeline.process(query)\n",
    "            print(\"\\nResponse:\")\n",
    "            print(response)\n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        await pipeline.close()\n",
    "\n",
    "# Run the main function if this script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b2070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports and async helper\n",
    "import asyncio\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "async def run_single_query(query: str):\n",
    "    \n",
    "    # SentenceTransformer embedding model must match the one used for ingest\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    # Wrap it in Chromas embedding interface\n",
    "    class EmbeddingFn:\n",
    "        def __init__(self, model): self.model = model\n",
    "        def __call__(self, texts): return self.model.encode(texts).tolist()\n",
    "    \n",
    "    # 3. Instantiate your pipeline\n",
    "    pipeline = QueryPipeline(collection, embedding_model=embedding_model)\n",
    "    \n",
    "    # 4. Process the query and print the answer\n",
    "    response = await pipeline.process(query)\n",
    "    print(\"\\n=== Response ===\\n\", response)\n",
    "    \n",
    "    # 5. Clean up\n",
    "    await pipeline.close()\n",
    "\n",
    "# 6. Run it!\n",
    "query_text = \"What next steps were planned for Ganges International Pvt Ltd?\"\n",
    "asyncio.run(run_single_query(query_text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
